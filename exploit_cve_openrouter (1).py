#!/usr/bin/env python3
"""
Exploit-to-CVE Remediation Script (Groq)
- Reads a JSON file with "success": true and "exploit_name" (or list of findings).
- Uses Groq API to: map exploit_name -> CVE, get remediation, SIGMA rules/queries, endpoint mitigation.
- Writes results to a CSV file in I:\\A.K\\NTI\\Project 2\\CSV results
"""
 
import json
import csv
import os
import re
import sys
import argparse
import asyncio
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests


# Input JSON and output CSV directories
INPUT_JSON_DIR = Path(r'I:\A.K\NTI\Project 2')
OUTPUT_DIR = Path(r'C:\Users\hoor0\OneDrive\Desktop\project phase2')
GROQ_URL = 'https://api.groq.com/openai/v1/chat/completions'
DEFAULT_MODEL = os.environ.get('GROQ_MODEL', 'llama-3.3-70b-versatile')

# Platform webhook URL for sending CVE results to the dashboard
PLATFORM_WEBHOOK_URL = os.environ.get('PLATFORM_WEBHOOK_URL', 'http://localhost:3000/api/webhook/cve-results')


def get_api_key() -> str:
    key = os.environ.get('GROQ_API_KEY', '').strip()
    if not key:
        raise ValueError(
            'Groq API key not set. Please set the GROQ_API_KEY environment variable. '
            'Get a key at: https://console.groq.com/keys'
        )
    return key


def groq_chat(
    api_key: str,
    user_content: str,
    model: str = DEFAULT_MODEL,
    max_tokens: int = 2048,
    temperature: float = 0.3,
) -> str:
    """Send a chat completion request to Groq and return the assistant text."""
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json',
    }
    # Groq prefers max_completion_tokens; keep payload minimal to avoid 400
    payload = {
        'model': model,
        'messages': [{'role': 'user', 'content': user_content}],
        'max_completion_tokens': min(max_tokens, 8192),
        'temperature': temperature,
    }
    resp = requests.post(GROQ_URL, headers=headers, json=payload, timeout=120)
    if resp.status_code != 200:
        err = resp.text
        try:
            err = resp.json()
        except Exception:
            pass
        raise RuntimeError(f'Groq API error {resp.status_code}: {err}')
    data = resp.json()
    choice = data.get('choices')
    if not choice:
        raise RuntimeError('Groq returned no choices')
    content = choice[0].get('message', {}).get('content') or ''
    return content.strip()


def extract_cve_from_response(text: str) -> str:
    """Try to extract a CVE ID from model response (e.g. CVE-2024-1234)."""
    match = re.search(r'CVE-\d{4}-\d{4,}', text, re.IGNORECASE)
    return match.group(0) if match else text.strip()[:200]


def _collect_all_success_objects(data: Any, seen: set) -> List[Dict[str, Any]]:
    """Recursively collect every dict in the JSON that has "success": true (no duplicates)."""
    out: List[Dict[str, Any]] = []
    if isinstance(data, dict):
        if data.get('success') is True and id(data) not in seen:
            seen.add(id(data))
            out.append(data)
        for v in data.values():
            out.extend(_collect_all_success_objects(v, seen))
    elif isinstance(data, list):
        for item in data:
            out.extend(_collect_all_success_objects(item, seen))
    return out


def find_all_success_objects(data: Any) -> List[Dict[str, Any]]:
    """
    Find all objects in the JSON that have "success": true (top-level, nested, or in arrays).
    Returns a list of all such objects for use in the pipeline.
    """
    return _collect_all_success_objects(data, set())


def collect_exploit_names(data: Dict[str, Any]) -> List[str]:
    """
    Collect exploit_name(s) from JSON.
    Supports: "exploit_name" (str or list), or "findings"/"results" with exploit_name per item.
    """
    names = []
    if 'exploit_name' in data:
        val = data['exploit_name']
        if isinstance(val, str) and val.strip():
            names.append(val.strip())
        elif isinstance(val, list):
            for v in val:
                if isinstance(v, str) and v.strip():
                    names.append(v.strip())
                elif isinstance(v, dict) and v.get('exploit_name'):
                    names.append(str(v['exploit_name']).strip())
    for key in ('findings', 'results', 'exploits', 'items'):
        if key not in data or not isinstance(data[key], list):
            continue
        for item in data[key]:
            if isinstance(item, dict) and item.get('exploit_name'):
                names.append(str(item['exploit_name']).strip())
    return list(dict.fromkeys(names))  # unique, order preserved


def map_exploit_to_cve(api_key: str, exploit_name: str, model: str) -> str:
    prompt = (
        f'Map this security exploit or vulnerability name to its official CVE identifier(s). '
        f'Exploit name: "{exploit_name}". '
        f'Reply with only the CVE ID(s), one per line if multiple (e.g. CVE-2024-1234). '
        f'If no CVE exists, reply with "N/A". Do not add explanation.'
    )
    reply = groq_chat(api_key, prompt, model=model, max_tokens=256)
    return extract_cve_from_response(reply)


def get_remediation_steps(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'Provide clear, actionable remediation steps for {cve_id}. '
        f'Use a numbered list. Include patching, configuration changes, and mitigation. '
        f'Keep the response concise but complete.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=1024)


def get_sigma_and_queries(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'For {cve_id}: (1) Write one or more SIGMA detection rules (YAML) that could detect exploitation. '
        f'(2) Then convert those SIGMA rules into concrete detection queries (e.g. KQL for Microsoft Defender, Splunk SPL, or generic SIEM query). '
        f'Output both the SIGMA rule(s) and the corresponding runnable queries clearly labeled.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=2048)


def get_endpoint_mitigation_commands(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'For {cve_id}, provide Endpoint mitigation commands that can be deployed on a compromised endpoint. '
        f'Include: Windows (PowerShell or CMD), and Linux (bash) where relevant. '
        f'Commands should be runnable (e.g. disable a service, apply a registry fix, block a path). '
        f'List each command with a short description. Do not use placeholders like <path> without example.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=2048)


def run_pipeline(
    json_path: Path,
    api_key: str,
    model: str,
    output_csv_path: Path,
    delay_between_calls: float = 0.5,
    discord_message_id: Optional[str] = None,
    discord_channel_id: Optional[str] = None,
) -> Path:
    with open(json_path, 'r', encoding='utf-8') as f:
        raw = json.load(f)

    success_objects = find_all_success_objects(raw)
    if not success_objects:
        raise ValueError(
            'JSON must contain at least one "success": true (at top level, or inside result/response/data, or in an array element).'
        )

    # Collect exploit_name from every success object; keep unique, order preserved
    seen_names: set = set()
    exploit_names: List[str] = []
    for obj in success_objects:
        for name in collect_exploit_names(obj):
            if name not in seen_names:
                seen_names.add(name)
                exploit_names.append(name)
    if not exploit_names:
        raise ValueError(
            'No exploit_name found. Expected "exploit_name" (string or list) or '
            '"findings"/"results" with "exploit_name" in each item.'
        )

    rows = []
    for i, exploit_name in enumerate(exploit_names):
        print(f'Processing ({i+1}/{len(exploit_names)}): {exploit_name}')
        time.sleep(delay_between_calls)

        cve_id = map_exploit_to_cve(api_key, exploit_name, model)
        time.sleep(delay_between_calls)

        remediation = get_remediation_steps(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        sigma_queries = get_sigma_and_queries(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        mitigation = get_endpoint_mitigation_commands(api_key, cve_id, model)

        rows.append({
            'exploit_name': exploit_name,
            'cve_id': cve_id,
            'remediation_steps': remediation,
            'sigma_detection_queries': sigma_queries,
            'endpoint_mitigation_commands': mitigation,
        })

    # Change output path from .csv to .txt
    output_txt_path = output_csv_path.with_suffix('.txt')
    output_txt_path.parent.mkdir(parents=True, exist_ok=True)

    def format_text_report(rows: List[Dict[str, Any]]) -> str:
        """Format results as a readable text report."""
        lines = []
        lines.append('=' * 80)
        lines.append('CVE ANALYSIS REPORT')
        lines.append(f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        lines.append(f'Total Exploits Analyzed: {len(rows)}')
        lines.append('=' * 80)
        lines.append('')
        
        for i, row in enumerate(rows, 1):
            lines.append(f'{"‚îÄ" * 80}')
            lines.append(f'[{i}] EXPLOIT: {row.get("exploit_name", "Unknown")}')
            lines.append(f'    CVE ID: {row.get("cve_id", "N/A")}')
            lines.append(f'{"‚îÄ" * 80}')
            lines.append('')
            
            # Remediation Steps
            lines.append('‚îå‚îÄ REMEDIATION STEPS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            remediation = row.get('remediation_steps', 'No remediation steps available.')
            for line in remediation.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            
            # SIGMA Rules
            lines.append('‚îå‚îÄ SIGMA DETECTION RULES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            sigma = row.get('sigma_detection_queries', 'No SIGMA rules available.')
            for line in sigma.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            
            # Endpoint Mitigation
            lines.append('‚îå‚îÄ ENDPOINT MITIGATION COMMANDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            mitigation = row.get('endpoint_mitigation_commands', 'No mitigation commands available.')
            for line in mitigation.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            lines.append('')
        
        lines.append('=' * 80)
        lines.append('END OF REPORT')
        lines.append('=' * 80)
        
        return '\n'.join(lines)

    def write_text(path: Path) -> None:
        with open(path, 'w', encoding='utf-8') as f:
            f.write(format_text_report(rows))

    try:
        write_text(output_txt_path)
    except (PermissionError, OSError) as e:
        stamped = output_txt_path.parent / (
            output_txt_path.stem + '_' + datetime.now().strftime('%Y%m%d_%H%M%S') + output_txt_path.suffix
        )
        try:
            write_text(stamped)
            print(f'[WARNING] Could not write to {output_txt_path}: {e}', file=sys.stderr)
            print(f'  Saved instead to: {stamped}', file=sys.stderr)
            output_txt_path = stamped
        except (PermissionError, OSError):
            fallback_dir = Path(__file__).resolve().parent / 'Text results'
            fallback_dir.mkdir(parents=True, exist_ok=True)
            fallback_path = fallback_dir / output_txt_path.name
            try:
                write_text(fallback_path)
                print(f'[WARNING] Could not write to {output_txt_path}: {e}', file=sys.stderr)
                print(f'  Saved instead to: {fallback_path}', file=sys.stderr)
                output_txt_path = fallback_path
            except (PermissionError, OSError):
                raise RuntimeError(
                    f'Permission denied writing to {output_txt_path}. '
                    'Close the file if it is open, or check folder permissions.'
                ) from e

    print(f'Written {len(rows)} result(s) to {output_txt_path}')
    
    # Send results to the Purple Team Platform dashboard
    send_results_to_platform(
        rows=rows,
        source='discord_bot' if discord_message_id else 'python_script',
        json_file=str(json_path.name),
        message_id=discord_message_id,
        channel_id=discord_channel_id,
    )
    
    return output_csv_path


def parse_discord_attack_message(message_content: str) -> Dict[str, Any]:
    """
    Parse a Discord message from Purple Team Platform attack results.
    
    Supported formats:
    - ‚úÖ üó°Ô∏è Metasploit Module ‚Äî SUCCESS
    - ‚úÖ üíª Custom Command ‚Äî SUCCESS  
    - ‚úÖ ‚öõÔ∏è Atomic Red Team Test ‚Äî SUCCESS
    
    Fields parsed:
    - üìã Attack Type: METASPLOIT / CUSTOM_COMMAND / ART_TEST
    - ‚úÖ Status: SUCCESS
    - üéØ Target: 192.168.1.5:21
    - üì¶ Module / Technique: exploit/unix/ftp/vsftpd_234_backdoor
    - üÜî Execution ID: msf_1770716457553
    """
    result = {
        'exploit_names': [],
        'attack_type': None,
        'target': None,
        'technique': None,
        'test_name': None,
        'execution_id': None,
        'status': None,
        'os_type': None,
        'executor': None,
        'command': None,
        'raw_content': message_content,
    }
    
    lines = message_content.split('\n')
    
    # Check first line for status and attack type
    if lines:
        first_line = lines[0].strip()
        if '‚úÖ' in first_line and 'SUCCESS' in first_line.upper():
            result['status'] = 'SUCCESS'
        elif '‚ùå' in first_line and 'FAILURE' in first_line.upper():
            result['status'] = 'FAILURE'
        
        # Detect attack type from first line
        if 'Metasploit' in first_line:
            result['attack_type'] = 'METASPLOIT'
        elif 'Custom Command' in first_line:
            result['attack_type'] = 'CUSTOM_COMMAND'
        elif 'Atomic Red Team' in first_line:
            result['attack_type'] = 'ART_TEST'
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        next_line = lines[i + 1].strip() if i + 1 < len(lines) else ''
        
        # Parse inline format (üìã Attack Type: VALUE) or multi-line format
        
        # Attack Type
        if 'üìã Attack Type' in line_stripped or 'üìã' in line_stripped and 'Attack Type' in line_stripped:
            if ':' in line_stripped:
                result['attack_type'] = line_stripped.split(':')[-1].strip()
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['attack_type'] = next_line
        
        # Status
        if '‚úÖ Status' in line_stripped or '‚ùå Status' in line_stripped:
            if next_line:
                result['status'] = next_line
        
        # Target
        if 'üéØ Target' in line_stripped or ('üéØ' in line_stripped and 'Target' in line_stripped):
            if ':' in line_stripped:
                parts = line_stripped.split(':')
                if len(parts) >= 2:
                    result['target'] = ':'.join(parts[1:]).strip()
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['target'] = next_line
        
        # Module / Technique - THIS IS THE KEY EXPLOIT NAME
        if 'üì¶ Module / Technique' in line_stripped or ('üì¶' in line_stripped and 'Module' in line_stripped):
            if ':' in line_stripped:
                parts = line_stripped.split(':')
                if len(parts) >= 2:
                    technique_str = ':'.join(parts[1:]).strip()
                    result['technique'] = technique_str
                    if technique_str and technique_str not in result['exploit_names']:
                        result['exploit_names'].append(technique_str)
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['technique'] = next_line
                if next_line and next_line not in result['exploit_names']:
                    result['exploit_names'].append(next_line)
        
        # Test Name (for ART tests)
        if 'üî¨ Test Name' in line_stripped or ('üî¨' in line_stripped and 'Test Name' in line_stripped):
            if ':' in line_stripped:
                parts = line_stripped.split(':')
                if len(parts) >= 2:
                    test_name = ':'.join(parts[1:]).strip()
                    result['test_name'] = test_name
                    if test_name and test_name not in result['exploit_names']:
                        result['exploit_names'].append(test_name)
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['test_name'] = next_line
                if next_line and next_line not in result['exploit_names']:
                    result['exploit_names'].append(next_line)
        
        # Execution ID
        if 'üÜî Execution ID' in line_stripped or ('üÜî' in line_stripped and 'Execution ID' in line_stripped):
            if ':' in line_stripped:
                parts = line_stripped.split(':')
                if len(parts) >= 2:
                    result['execution_id'] = ':'.join(parts[1:]).strip()
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['execution_id'] = next_line
        
        # Executor
        if '‚öôÔ∏è Executor' in line_stripped or ('‚öôÔ∏è' in line_stripped and 'Executor' in line_stripped):
            if ':' in line_stripped:
                result['executor'] = line_stripped.split(':')[-1].strip()
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['executor'] = next_line
        
        # OS Type
        if 'üñ•Ô∏è OS Type' in line_stripped or ('üñ•Ô∏è' in line_stripped and 'OS Type' in line_stripped):
            if ':' in line_stripped:
                result['os_type'] = line_stripped.split(':')[-1].strip()
            elif next_line and not next_line.startswith(('üìã', '‚úÖ', '‚ùå', 'üéØ', 'üì¶', 'üî¨', 'üÜî', '‚öôÔ∏è', 'üî¢', 'üìù', 'üñ•Ô∏è')):
                result['os_type'] = next_line
        
        # Command Executed
        if 'Command Executed' in line_stripped:
            # Collect command lines until we hit Output: or another section
            cmd_lines = []
            for j in range(i + 1, len(lines)):
                cmd_line = lines[j].strip()
                if cmd_line.startswith('Output:') or cmd_line.startswith('üìã') or cmd_line.startswith('‚úÖ') or cmd_line.startswith('‚ùå'):
                    break
                if cmd_line:
                    cmd_lines.append(cmd_line)
            if cmd_lines:
                result['command'] = '\n'.join(cmd_lines)
    
    return result


def run_pipeline_from_text(
    exploit_names: List[str],
    api_key: str,
    model: str,
    output_path: Path,
    delay_between_calls: float = 0.5,
    discord_message_id: Optional[str] = None,
    discord_channel_id: Optional[str] = None,
    source_text: Optional[str] = None,
) -> Path:
    """
    Run the CVE analysis pipeline from a list of exploit names (parsed from Discord text).
    Outputs a formatted text file instead of CSV.
    """
    if not exploit_names:
        raise ValueError('No exploit names provided.')
    
    # Remove duplicates while preserving order
    seen = set()
    unique_names = []
    for name in exploit_names:
        if name and name not in seen:
            seen.add(name)
            unique_names.append(name)
    
    if not unique_names:
        raise ValueError('No valid exploit names found.')
    
    rows = []
    for i, exploit_name in enumerate(unique_names):
        print(f'Processing ({i+1}/{len(unique_names)}): {exploit_name}')
        time.sleep(delay_between_calls)

        cve_id = map_exploit_to_cve(api_key, exploit_name, model)
        time.sleep(delay_between_calls)

        remediation = get_remediation_steps(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        sigma_queries = get_sigma_and_queries(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        mitigation = get_endpoint_mitigation_commands(api_key, cve_id, model)

        rows.append({
            'exploit_name': exploit_name,
            'cve_id': cve_id,
            'remediation_steps': remediation,
            'sigma_detection_queries': sigma_queries,
            'endpoint_mitigation_commands': mitigation,
        })

    # Change output to .txt format
    output_txt_path = output_path.with_suffix('.txt')
    output_txt_path.parent.mkdir(parents=True, exist_ok=True)

    def format_text_report(rows: List[Dict[str, Any]]) -> str:
        """Format results as a readable text report."""
        lines = []
        lines.append('=' * 80)
        lines.append('CVE ANALYSIS REPORT')
        lines.append(f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        lines.append(f'Total Exploits Analyzed: {len(rows)}')
        lines.append('=' * 80)
        lines.append('')
        
        for i, row in enumerate(rows, 1):
            lines.append(f'{"‚îÄ" * 80}')
            lines.append(f'[{i}] EXPLOIT: {row.get("exploit_name", "Unknown")}')
            lines.append(f'    CVE ID: {row.get("cve_id", "N/A")}')
            lines.append(f'{"‚îÄ" * 80}')
            lines.append('')
            
            lines.append('‚îå‚îÄ REMEDIATION STEPS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            remediation = row.get('remediation_steps', 'No remediation steps available.')
            for line in remediation.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            
            lines.append('‚îå‚îÄ SIGMA DETECTION RULES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            sigma = row.get('sigma_detection_queries', 'No SIGMA rules available.')
            for line in sigma.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            
            lines.append('‚îå‚îÄ ENDPOINT MITIGATION COMMANDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê')
            mitigation = row.get('endpoint_mitigation_commands', 'No mitigation commands available.')
            for line in mitigation.split('\n'):
                lines.append(f'‚îÇ {line}')
            lines.append('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò')
            lines.append('')
            lines.append('')
        
        lines.append('=' * 80)
        lines.append('END OF REPORT')
        lines.append('=' * 80)
        
        return '\n'.join(lines)

    def write_text(path: Path) -> None:
        with open(path, 'w', encoding='utf-8') as f:
            f.write(format_text_report(rows))

    try:
        write_text(output_txt_path)
    except (PermissionError, OSError) as e:
        stamped = output_txt_path.parent / (
            output_txt_path.stem + '_' + datetime.now().strftime('%Y%m%d_%H%M%S') + output_txt_path.suffix
        )
        try:
            write_text(stamped)
            print(f'[WARNING] Could not write to {output_txt_path}: {e}', file=sys.stderr)
            print(f'  Saved instead to: {stamped}', file=sys.stderr)
            output_txt_path = stamped
        except (PermissionError, OSError):
            fallback_dir = Path(__file__).resolve().parent / 'Text results'
            fallback_dir.mkdir(parents=True, exist_ok=True)
            fallback_path = fallback_dir / output_txt_path.name
            try:
                write_text(fallback_path)
                print(f'[WARNING] Could not write to {output_txt_path}: {e}', file=sys.stderr)
                print(f'  Saved instead to: {fallback_path}', file=sys.stderr)
                output_txt_path = fallback_path
            except (PermissionError, OSError):
                raise RuntimeError(
                    f'Permission denied writing to {output_txt_path}. '
                    'Close the file if it is open, or check folder permissions.'
                ) from e

    print(f'Written {len(rows)} result(s) to {output_txt_path}')
    
    # Send results to the Purple Team Platform dashboard
    send_results_to_platform(
        rows=rows,
        source='discord_bot' if discord_message_id else 'python_script',
        json_file=source_text[:100] + '...' if source_text and len(source_text) > 100 else source_text,
        message_id=discord_message_id,
        channel_id=discord_channel_id,
    )
    
    return output_txt_path


def upload_file_to_discord_webhook(webhook_url: str, file_path: Path, content: str = '') -> None:
    """
    Upload a file to an incoming Discord webhook (write-only).

    Notes:
    - Webhooks cannot read channel history. Use a bot token to *download* attachments.
    """
    with open(file_path, 'rb') as f:
        files = {'file': (file_path.name, f)}
        data = {'content': content} if content else {}
        resp = requests.post(webhook_url, data=data, files=files, timeout=120)
    resp.raise_for_status()


def send_results_to_platform(
    rows: List[Dict[str, Any]],
    platform_url: str = PLATFORM_WEBHOOK_URL,
    source: str = 'python_script',
    json_file: Optional[str] = None,
    message_id: Optional[str] = None,
    channel_id: Optional[str] = None,
) -> bool:
    """
    Send CVE analysis results to the Purple Team Platform dashboard via webhook.
    Sends to both the CVE results endpoint AND the analysis-import endpoint
    to populate all 3 dashboard tabs (Remediation, SIEM Rules, AI Ingest).
    
    Args:
        rows: List of CVE analysis results (each with exploit_name, cve_id, etc.)
        platform_url: The platform webhook URL
        source: Source identifier ('python_script' or 'discord_bot')
        json_file: Optional source JSON filename
        message_id: Optional Discord message ID
        channel_id: Optional Discord channel ID
    
    Returns:
        True if successful, False otherwise
    """
    if not rows:
        print('[PLATFORM] No results to send')
        return False
    
    execution_id = f'cve_{int(time.time())}_{os.urandom(4).hex()}'
    timestamp = datetime.now().isoformat()
    
    payload = {
        'source': source,
        'execution_id': execution_id,
        'timestamp': timestamp,
        'results': [
            {
                'exploit_name': row.get('exploit_name', ''),
                'cve_id': row.get('cve_id', ''),
                'remediation_steps': row.get('remediation_steps', ''),
                'sigma_detection_queries': row.get('sigma_detection_queries', ''),
                'endpoint_mitigation_commands': row.get('endpoint_mitigation_commands', ''),
            }
            for row in rows
        ],
        'metadata': {},
    }
    
    if json_file:
        payload['metadata']['json_file'] = json_file
    if message_id:
        payload['metadata']['message_id'] = message_id
    if channel_id:
        payload['metadata']['channel_id'] = channel_id
    
    success = False
    
    # Send to CVE results endpoint
    try:
        resp = requests.post(
            platform_url,
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=30,
        )
        if resp.status_code == 200:
            result = resp.json()
            print(f'[PLATFORM] CVE results sent. Execution ID: {result.get("execution_id", "unknown")}')
            success = True
        else:
            print(f'[PLATFORM] Failed to send CVE results: {resp.status_code}', file=sys.stderr)
    except requests.exceptions.RequestException as e:
        print(f'[PLATFORM] Error sending CVE results: {e}', file=sys.stderr)
    
    # Also send to analysis-import endpoint to populate all 3 dashboard tabs
    analysis_import_url = platform_url.replace('/cve-results', '/analysis-import')
    try:
        resp = requests.post(
            analysis_import_url,
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=30,
        )
        if resp.status_code == 200:
            result = resp.json()
            print(f'[PLATFORM] Full analysis created for Remediation/SIEM/Ingest tabs')
            success = True
        else:
            print(f'[PLATFORM] Failed to create full analysis: {resp.status_code}', file=sys.stderr)
    except requests.exceptions.RequestException as e:
        print(f'[PLATFORM] Error creating full analysis: {e}', file=sys.stderr)
    
    return success


async def run_discord_listener(
    discord_bot_token: str,
    source_channel_id: int,
    output_dir: Path,
    groq_api_key: str,
    groq_model: str,
    delay_between_calls: float,
    dest_webhook_url: Optional[str] = None,
    downloads_dir: Optional[Path] = None,
    source_channel_ids: Optional[List[int]] = None,
) -> None:
    """
    Listen for new .json attachments in a Discord channel, download them, run the pipeline,
    and optionally upload the resulting CSV via a Discord webhook.

    Required environment variables (recommended):
    - DISCORD_BOT_TOKEN
    - GROQ_API_KEY

    Optional:
    - DISCORD_DEST_WEBHOOK_URL (to upload CSV to another channel via webhook)
    - DISCORD_SOURCE_CHANNEL_ID (defaults can be set via CLI too)
    """
    try:
        import discord  # type: ignore
    except Exception as e:
        raise RuntimeError(
            'discord.py is required for Discord listener mode. Install it with: pip install -U discord.py'
        ) from e

    downloads_dir = downloads_dir or (output_dir / 'discord_downloads')
    downloads_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    intents = discord.Intents.default()
    # Attachments come via MessageCreate; message content intent is not strictly required for attachments,
    # but enabling it avoids surprises depending on server settings.
    intents.message_content = True

    client = discord.Client(intents=intents)
    processing_lock = asyncio.Lock()

    watched_channels = set(source_channel_ids or [source_channel_id])

    @client.event
    async def on_ready():
        print(f'Logged in as {client.user} (id={getattr(client.user, "id", "unknown")})')
        if len(watched_channels) == 1:
            print(f'Watching source channel: {next(iter(watched_channels))}')
        else:
            print(f'Watching source channels: {sorted(watched_channels)}')
        if dest_webhook_url:
            print('CSV upload via webhook: enabled')
        else:
            print('CSV upload via webhook: disabled (no webhook URL set)')

    @client.event
    async def on_message(message):  # type: ignore[no-redef]
        # Debug: Log all incoming messages
        incoming_channel_id = getattr(getattr(message, 'channel', None), 'id', None)
        author_id = getattr(getattr(message, 'author', None), 'id', None)
        author_name = getattr(getattr(message, 'author', None), 'name', 'unknown')
        bot_id = getattr(getattr(client, 'user', None), 'id', None)
        print(f'[DEBUG] Message received from channel {incoming_channel_id} by {author_name} (id={author_id})')
        
        # Only ignore if author ID matches bot ID (not by object comparison)
        if author_id and bot_id and author_id == bot_id:
            print('[DEBUG] Ignoring: message from self')
            return
        if incoming_channel_id not in watched_channels:
            print(f'[DEBUG] Ignoring: channel {incoming_channel_id} not in watched channels {sorted(watched_channels)}')
            return

        msg_id = str(getattr(message, 'id', ''))
        chan_id = str(incoming_channel_id)
        message_content = getattr(message, 'content', '') or ''
        
        # Debug: Log message details
        attachments = getattr(message, 'attachments', []) or []
        print(f'[DEBUG] Processing message {msg_id}: {len(attachments)} attachment(s), content length: {len(message_content)}')
        
        # Check for JSON attachments first
        attachments = getattr(message, 'attachments', []) or []
        json_attachments = [a for a in attachments if str(getattr(a, 'filename', '')).lower().endswith('.json')]
        
        if json_attachments:
            # Process JSON attachment (original behavior)
            att = json_attachments[0]
            original_name = Path(getattr(att, 'filename', 'input.json')).name
            safe_stem = Path(original_name).stem
            suffix = Path(original_name).suffix or '.json'
            unique_name = f'{safe_stem}_{msg_id}{suffix}'
            json_path = downloads_dir / unique_name

            async with processing_lock:
                try:
                    print(f'\n[DISCORD] Downloading: {original_name} (message_id={msg_id})')
                    await att.save(json_path)
                    print(f'[DISCORD] Saved JSON: {json_path}')

                    output_csv_path = output_dir / f'{json_path.stem}_cve_remediation.csv'

                    csv_path = await asyncio.to_thread(
                        run_pipeline,
                        json_path,
                        groq_api_key,
                        groq_model,
                        output_csv_path,
                        delay_between_calls,
                        msg_id,
                        chan_id,
                    )

                    print(f'[DISCORD] Generated CSV: {csv_path}')

                    if dest_webhook_url:
                        upload_file_to_discord_webhook(
                            dest_webhook_url,
                            csv_path,
                            content=f'Generated from `{original_name}` (message_id={msg_id})',
                        )
                        print('[DISCORD] Uploaded CSV via webhook')
                except Exception as e:
                    print(f'[DISCORD][ERROR] {e}', file=sys.stderr)
        
        elif message_content.strip():
            # Process text message - look for SUCCESS attack results only
            # Check if message contains SUCCESS indicator
            is_success = '‚úÖ' in message_content or 'SUCCESS' in message_content.upper()
            
            if not is_success:
                print(f'[DEBUG] Ignoring: not a SUCCESS message')
                return  # Only process SUCCESS messages
            
            # Check if message looks like an attack result (contains key indicators)
            indicators = ['Atomic Red Team', 'Metasploit', 'Custom Command', 'Attack Type', 'Module / Technique', 'Execution ID', 'üìã', 'üéØ', 'üì¶', 'üÜî', 'METASPLOIT', 'CUSTOM_COMMAND', 'ART_TEST']
            has_indicator = any(ind in message_content for ind in indicators)
            
            if not has_indicator:
                print(f'[DEBUG] Ignoring: no attack indicators found')
                return  # Not an attack result message, ignore
            
            async with processing_lock:
                try:
                    print(f'\n[DISCORD] Processing text message (message_id={msg_id})')
                    
                    # Parse the message to extract exploit names
                    parsed = parse_discord_attack_message(message_content)
                    exploit_names = parsed.get('exploit_names', [])
                    
                    if not exploit_names:
                        print(f'[DISCORD] No exploit names found in message, skipping.')
                        return
                    
                    print(f'[DISCORD] Found exploit names: {exploit_names}')
                    
                    # Generate unique output filename
                    execution_id = parsed.get('execution_id') or msg_id
                    output_csv_path = output_dir / f'discord_text_{execution_id}_cve_remediation.csv'
                    
                    csv_path = await asyncio.to_thread(
                        run_pipeline_from_text,
                        exploit_names,
                        groq_api_key,
                        groq_model,
                        output_csv_path,
                        delay_between_calls,
                        msg_id,
                        chan_id,
                        message_content[:500],  # First 500 chars as source reference
                    )

                    print(f'[DISCORD] Generated CSV: {csv_path}')

                    if dest_webhook_url:
                        technique = parsed.get('technique') or 'text message'
                        upload_file_to_discord_webhook(
                            dest_webhook_url,
                            csv_path,
                            content=f'Generated from text message: `{technique}` (message_id={msg_id})',
                        )
                        print('[DISCORD] Uploaded CSV via webhook')
                except Exception as e:
                    print(f'[DISCORD][ERROR] {e}', file=sys.stderr)

    await client.start(discord_bot_token)


def main():
    parser = argparse.ArgumentParser(
        description='Map exploit names to CVE, get remediation, SIGMA queries, and mitigation via Groq; output CSV to I:\\A.K\\NTI\\Project 2\\CSV results',
    )
    parser.add_argument('json_file', type=str, nargs='?', default=None, help='Input JSON file (CLI mode). If omitted, Discord bot mode can be used.')
    parser.add_argument('-o', '--output', type=str, default=None, help='Output CSV filename (default: <json_stem>_cve_remediation.csv in Project 2 folder)')
    parser.add_argument('-m', '--model', type=str, default=DEFAULT_MODEL, help=f'Groq model (default: {DEFAULT_MODEL})')
    parser.add_argument('--delay', type=float, default=0.5, help='Seconds between API calls (default: 0.5)')
    parser.add_argument('--discord', action='store_true', help='Run as a Discord listener bot (download .json attachments and process automatically).')
    parser.add_argument(
        '--source-channel-id',
        type=int,
        default=int(os.environ.get('DISCORD_SOURCE_CHANNEL_ID', '1469327593022161073')),
        help='Discord channel ID to watch (legacy single-channel option; default: env DISCORD_SOURCE_CHANNEL_ID or 1469327593022161073).',
    )
    parser.add_argument(
        '--source-channel-ids',
        type=str,
        default=os.environ.get('DISCORD_SOURCE_CHANNEL_IDS', '').strip() or None,
        help='Comma-separated Discord channel IDs to watch (default: env DISCORD_SOURCE_CHANNEL_IDS). If set, overrides --source-channel-id.',
    )
    parser.add_argument(
        '--dest-webhook-url',
        type=str,
        default=os.environ.get('DISCORD_DEST_WEBHOOK_URL', '').strip() or None,
        help='Discord incoming webhook URL to upload CSV results (default: env DISCORD_DEST_WEBHOOK_URL).',
    )
    parser.add_argument(
        '--downloads-dir',
        type=str,
        default=os.environ.get('DISCORD_DOWNLOADS_DIR', '').strip() or None,
        help='Folder to store downloaded JSON attachments (default: OUTPUT_DIR\\discord_downloads).',
    )
    args = parser.parse_args()

    # Prefer Discord mode when explicitly requested, or when no JSON file is given but a bot token exists.
    discord_bot_token = os.environ.get('DISCORD_BOT_TOKEN', '').strip()
    should_run_discord = bool(args.discord or ((args.json_file is None or not str(args.json_file).strip()) and discord_bot_token))

    json_file_path = args.json_file
    if should_run_discord:
        if not discord_bot_token:
            print('[ERROR] DISCORD_BOT_TOKEN is not set in environment.', file=sys.stderr)
            sys.exit(1)
        try:
            api_key = get_api_key()
        except ValueError as e:
            print(f'[ERROR] {e}', file=sys.stderr)
            sys.exit(1)

        downloads_dir = Path(args.downloads_dir) if args.downloads_dir else (OUTPUT_DIR / 'discord_downloads')
        try:
            source_ids: Optional[List[int]] = None
            if getattr(args, 'source_channel_ids', None):
                source_ids = [int(x.strip()) for x in str(args.source_channel_ids).split(',') if x.strip()]

            asyncio.run(
                run_discord_listener(
                    discord_bot_token=discord_bot_token,
                    source_channel_id=int(args.source_channel_id),
                    source_channel_ids=source_ids,
                    output_dir=OUTPUT_DIR,
                    groq_api_key=api_key,
                    groq_model=args.model,
                    delay_between_calls=args.delay,
                    downloads_dir=Path(args.downloads_dir) if args.downloads_dir else None,
                    dest_webhook_url=args.dest_webhook_url,
                )
            )
        except KeyboardInterrupt:
            print('\n[INFO] Stopped by user.')
        return

    if json_file_path is None or not str(json_file_path).strip():
        print('[ERROR] No JSON file path provided. Provide a JSON file argument, or run with --discord.', file=sys.stderr)
        sys.exit(1)

    json_path = Path(json_file_path)
    if not json_path.is_file():
        # If only a filename was given, search in INPUT_JSON_DIR first, then OUTPUT_DIR, script dir, cwd
        if not json_path.is_absolute() and len(json_path.parts) <= 1:
            script_dir = Path(__file__).resolve().parent
            for base in (INPUT_JSON_DIR, OUTPUT_DIR, script_dir, Path.cwd()):
                candidate = base / json_path
                if candidate.is_file():
                    json_path = candidate
                    break
        if not json_path.is_file():
            print(f'[ERROR] File not found: {json_file_path}', file=sys.stderr)
            print(f'  Searched in: {INPUT_JSON_DIR}, {OUTPUT_DIR}, script directory, and current directory.', file=sys.stderr)
            print('  Tip: Use the full path to the file if it is elsewhere.', file=sys.stderr)
            sys.exit(1)

    if args.output:
        output_csv_path = Path(args.output)
        if not output_csv_path.is_absolute():
            output_csv_path = OUTPUT_DIR / output_csv_path.name
    else:
        output_csv_path = OUTPUT_DIR / f'{json_path.stem}_cve_remediation.csv'

    try:
        api_key = get_api_key()
    except ValueError as e:
        print(f'[ERROR] {e}', file=sys.stderr)
        sys.exit(1)

    try:
        run_pipeline(json_path, api_key, args.model, output_csv_path, delay_between_calls=args.delay)
        print(f'\n[SUCCESS] Output CSV: {output_csv_path}')
    except Exception as e:
        print(f'[ERROR] {e}', file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
