#!/usr/bin/env python3
"""
Exploit-to-CVE Remediation Script (Groq)
- Reads a JSON file with "success": true and "exploit_name" (or list of findings).
- Uses Groq API to: map exploit_name -> CVE, get remediation, SIGMA rules/queries, endpoint mitigation.
- Writes results to a CSV file in I:\\A.K\\NTI\\Project 2\\CSV results
"""
 
import json
import csv
import os
import re
import sys
import argparse
import asyncio
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests


# Input JSON and output CSV directories
INPUT_JSON_DIR = Path(r'I:\A.K\NTI\Project 2')
OUTPUT_DIR = Path(r'C:\Users\hoor0\OneDrive\Desktop\project phase2')
GROQ_URL = 'https://api.groq.com/openai/v1/chat/completions'
DEFAULT_MODEL = os.environ.get('GROQ_MODEL', 'llama-3.3-70b-versatile')


def get_api_key() -> str:
    key = os.environ.get('GROQ_API_KEY', '').strip()
    if not key:
        raise ValueError(
            'Groq API key not set. Please set the GROQ_API_KEY environment variable. '
            'Get a key at: https://console.groq.com/keys'
        )
    return key


def groq_chat(
    api_key: str,
    user_content: str,
    model: str = DEFAULT_MODEL,
    max_tokens: int = 2048,
    temperature: float = 0.3,
) -> str:
    """Send a chat completion request to Groq and return the assistant text."""
    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json',
    }
    # Groq prefers max_completion_tokens; keep payload minimal to avoid 400
    payload = {
        'model': model,
        'messages': [{'role': 'user', 'content': user_content}],
        'max_completion_tokens': min(max_tokens, 8192),
        'temperature': temperature,
    }
    resp = requests.post(GROQ_URL, headers=headers, json=payload, timeout=120)
    if resp.status_code != 200:
        err = resp.text
        try:
            err = resp.json()
        except Exception:
            pass
        raise RuntimeError(f'Groq API error {resp.status_code}: {err}')
    data = resp.json()
    choice = data.get('choices')
    if not choice:
        raise RuntimeError('Groq returned no choices')
    content = choice[0].get('message', {}).get('content') or ''
    return content.strip()


def extract_cve_from_response(text: str) -> str:
    """Try to extract a CVE ID from model response (e.g. CVE-2024-1234)."""
    match = re.search(r'CVE-\d{4}-\d{4,}', text, re.IGNORECASE)
    return match.group(0) if match else text.strip()[:200]


def _collect_all_success_objects(data: Any, seen: set) -> List[Dict[str, Any]]:
    """Recursively collect every dict in the JSON that has "success": true (no duplicates)."""
    out: List[Dict[str, Any]] = []
    if isinstance(data, dict):
        if data.get('success') is True and id(data) not in seen:
            seen.add(id(data))
            out.append(data)
        for v in data.values():
            out.extend(_collect_all_success_objects(v, seen))
    elif isinstance(data, list):
        for item in data:
            out.extend(_collect_all_success_objects(item, seen))
    return out


def find_all_success_objects(data: Any) -> List[Dict[str, Any]]:
    """
    Find all objects in the JSON that have "success": true (top-level, nested, or in arrays).
    Returns a list of all such objects for use in the pipeline.
    """
    return _collect_all_success_objects(data, set())


def collect_exploit_names(data: Dict[str, Any]) -> List[str]:
    """
    Collect exploit_name(s) from JSON.
    Supports: "exploit_name" (str or list), or "findings"/"results" with exploit_name per item.
    """
    names = []
    if 'exploit_name' in data:
        val = data['exploit_name']
        if isinstance(val, str) and val.strip():
            names.append(val.strip())
        elif isinstance(val, list):
            for v in val:
                if isinstance(v, str) and v.strip():
                    names.append(v.strip())
                elif isinstance(v, dict) and v.get('exploit_name'):
                    names.append(str(v['exploit_name']).strip())
    for key in ('findings', 'results', 'exploits', 'items'):
        if key not in data or not isinstance(data[key], list):
            continue
        for item in data[key]:
            if isinstance(item, dict) and item.get('exploit_name'):
                names.append(str(item['exploit_name']).strip())
    return list(dict.fromkeys(names))  # unique, order preserved


def map_exploit_to_cve(api_key: str, exploit_name: str, model: str) -> str:
    prompt = (
        f'Map this security exploit or vulnerability name to its official CVE identifier(s). '
        f'Exploit name: "{exploit_name}". '
        f'Reply with only the CVE ID(s), one per line if multiple (e.g. CVE-2024-1234). '
        f'If no CVE exists, reply with "N/A". Do not add explanation.'
    )
    reply = groq_chat(api_key, prompt, model=model, max_tokens=256)
    return extract_cve_from_response(reply)


def get_remediation_steps(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'Provide clear, actionable remediation steps for {cve_id}. '
        f'Use a numbered list. Include patching, configuration changes, and mitigation. '
        f'Keep the response concise but complete.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=1024)


def get_sigma_and_queries(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'For {cve_id}: (1) Write one or more SIGMA detection rules (YAML) that could detect exploitation. '
        f'(2) Then convert those SIGMA rules into concrete detection queries (e.g. KQL for Microsoft Defender, Splunk SPL, or generic SIEM query). '
        f'Output both the SIGMA rule(s) and the corresponding runnable queries clearly labeled.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=2048)


def get_endpoint_mitigation_commands(api_key: str, cve_id: str, model: str) -> str:
    prompt = (
        f'For {cve_id}, provide Endpoint mitigation commands that can be deployed on a compromised endpoint. '
        f'Include: Windows (PowerShell or CMD), and Linux (bash) where relevant. '
        f'Commands should be runnable (e.g. disable a service, apply a registry fix, block a path). '
        f'List each command with a short description. Do not use placeholders like <path> without example.'
    )
    return groq_chat(api_key, prompt, model=model, max_tokens=2048)


def run_pipeline(
    json_path: Path,
    api_key: str,
    model: str,
    output_csv_path: Path,
    delay_between_calls: float = 0.5,
) -> Path:
    with open(json_path, 'r', encoding='utf-8') as f:
        raw = json.load(f)

    success_objects = find_all_success_objects(raw)
    if not success_objects:
        raise ValueError(
            'JSON must contain at least one "success": true (at top level, or inside result/response/data, or in an array element).'
        )

    # Collect exploit_name from every success object; keep unique, order preserved
    seen_names: set = set()
    exploit_names: List[str] = []
    for obj in success_objects:
        for name in collect_exploit_names(obj):
            if name not in seen_names:
                seen_names.add(name)
                exploit_names.append(name)
    if not exploit_names:
        raise ValueError(
            'No exploit_name found. Expected "exploit_name" (string or list) or '
            '"findings"/"results" with "exploit_name" in each item.'
        )

    rows = []
    for i, exploit_name in enumerate(exploit_names):
        print(f'Processing ({i+1}/{len(exploit_names)}): {exploit_name}')
        time.sleep(delay_between_calls)

        cve_id = map_exploit_to_cve(api_key, exploit_name, model)
        time.sleep(delay_between_calls)

        remediation = get_remediation_steps(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        sigma_queries = get_sigma_and_queries(api_key, cve_id, model)
        time.sleep(delay_between_calls)

        mitigation = get_endpoint_mitigation_commands(api_key, cve_id, model)

        rows.append({
            'exploit_name': exploit_name,
            'cve_id': cve_id,
            'remediation_steps': remediation,
            'sigma_detection_queries': sigma_queries,
            'endpoint_mitigation_commands': mitigation,
        })

    output_csv_path.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        'exploit_name',
        'cve_id',
        'remediation_steps',
        'sigma_detection_queries',
        'endpoint_mitigation_commands',
    ]

    def write_csv(path: Path) -> None:
        with open(path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)

    try:
        write_csv(output_csv_path)
    except (PermissionError, OSError) as e:
        # File may be open in Excel; try same folder with timestamped name, then script folder
        stamped = output_csv_path.parent / (
            output_csv_path.stem + '_' + datetime.now().strftime('%Y%m%d_%H%M%S') + output_csv_path.suffix
        )
        try:
            write_csv(stamped)
            print(f'[WARNING] Could not write to {output_csv_path}: {e}', file=sys.stderr)
            print(f'  Saved instead to: {stamped}', file=sys.stderr)
            output_csv_path = stamped
        except (PermissionError, OSError):
            fallback_dir = Path(__file__).resolve().parent / 'CSV results'
            fallback_dir.mkdir(parents=True, exist_ok=True)
            fallback_path = fallback_dir / output_csv_path.name
            try:
                write_csv(fallback_path)
                print(f'[WARNING] Could not write to {output_csv_path}: {e}', file=sys.stderr)
                print(f'  Saved instead to: {fallback_path}', file=sys.stderr)
                output_csv_path = fallback_path
            except (PermissionError, OSError):
                raise RuntimeError(
                    f'Permission denied writing to {output_csv_path}. '
                    'Close the CSV file if it is open (e.g. in Excel), or check folder permissions.'
                ) from e

    print(f'Written {len(rows)} row(s) to {output_csv_path}')
    return output_csv_path


def upload_file_to_discord_webhook(webhook_url: str, file_path: Path, content: str = '') -> None:
    """
    Upload a file to an incoming Discord webhook (write-only).

    Notes:
    - Webhooks cannot read channel history. Use a bot token to *download* attachments.
    """
    with open(file_path, 'rb') as f:
        files = {'file': (file_path.name, f)}
        data = {'content': content} if content else {}
        resp = requests.post(webhook_url, data=data, files=files, timeout=120)
    resp.raise_for_status()


async def run_discord_listener(
    discord_bot_token: str,
    source_channel_id: int,
    output_dir: Path,
    groq_api_key: str,
    groq_model: str,
    delay_between_calls: float,
    dest_webhook_url: Optional[str],
    downloads_dir: Optional[Path] = None,
) -> None:
    """
    Listen for new .json attachments in a Discord channel, download them, run the pipeline,
    and optionally upload the resulting CSV via a Discord webhook.

    Required environment variables (recommended):
    - DISCORD_BOT_TOKEN
    - GROQ_API_KEY

    Optional:
    - DISCORD_DEST_WEBHOOK_URL (to upload CSV to another channel via webhook)
    - DISCORD_SOURCE_CHANNEL_ID (defaults can be set via CLI too)
    """
    try:
        import discord  # type: ignore
    except Exception as e:
        raise RuntimeError(
            'discord.py is required for Discord listener mode. Install it with: pip install -U discord.py'
        ) from e

    downloads_dir = downloads_dir or (output_dir / 'discord_downloads')
    downloads_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    intents = discord.Intents.default()
    # Attachments come via MessageCreate; message content intent is not strictly required for attachments,
    # but enabling it avoids surprises depending on server settings.
    intents.message_content = True

    client = discord.Client(intents=intents)
    processing_lock = asyncio.Lock()

    @client.event
    async def on_ready():
        print(f'Logged in as {client.user} (id={getattr(client.user, "id", "unknown")})')
        print(f'Watching source channel: {source_channel_id}')
        if dest_webhook_url:
            print('CSV upload via webhook: enabled')
        else:
            print('CSV upload via webhook: disabled (no webhook URL set)')

    @client.event
    async def on_message(message):  # type: ignore[no-redef]
        if getattr(message, 'author', None) == getattr(client, 'user', None):
            return
        if getattr(getattr(message, 'channel', None), 'id', None) != source_channel_id:
            return

        attachments = getattr(message, 'attachments', []) or []
        json_attachments = [a for a in attachments if str(getattr(a, 'filename', '')).lower().endswith('.json')]
        if not json_attachments:
            return

        att = json_attachments[0]
        original_name = Path(getattr(att, 'filename', 'input.json')).name
        safe_stem = Path(original_name).stem
        suffix = Path(original_name).suffix or '.json'
        # Make filename unique per message to avoid collisions
        unique_name = f'{safe_stem}_{getattr(message, "id", "msg")}{suffix}'
        json_path = downloads_dir / unique_name

        async with processing_lock:
            try:
                print(f'\n[DISCORD] Downloading: {original_name} (message_id={getattr(message, "id", "")})')
                await att.save(json_path)
                print(f'[DISCORD] Saved JSON: {json_path}')

                output_csv_path = output_dir / f'{json_path.stem}_cve_remediation.csv'

                # Run the (blocking) pipeline in a worker thread so the bot remains responsive.
                csv_path = await asyncio.to_thread(
                    run_pipeline,
                    json_path,
                    groq_api_key,
                    groq_model,
                    output_csv_path,
                    delay_between_calls,
                )

                print(f'[DISCORD] Generated CSV: {csv_path}')

                if dest_webhook_url:
                    upload_file_to_discord_webhook(
                        dest_webhook_url,
                        csv_path,
                        content=f'Generated from `{original_name}` (message_id={getattr(message, "id", "")})',
                    )
                    print('[DISCORD] Uploaded CSV via webhook')
            except Exception as e:
                print(f'[DISCORD][ERROR] {e}', file=sys.stderr)

    await client.start(discord_bot_token)


def main():
    parser = argparse.ArgumentParser(
        description='Map exploit names to CVE, get remediation, SIGMA queries, and mitigation via Groq; output CSV to I:\\A.K\\NTI\\Project 2\\CSV results',
    )
    parser.add_argument('json_file', type=str, nargs='?', default=None, help='Input JSON file (CLI mode). If omitted, Discord bot mode can be used.')
    parser.add_argument('-o', '--output', type=str, default=None, help='Output CSV filename (default: <json_stem>_cve_remediation.csv in Project 2 folder)')
    parser.add_argument('-m', '--model', type=str, default=DEFAULT_MODEL, help=f'Groq model (default: {DEFAULT_MODEL})')
    parser.add_argument('--delay', type=float, default=0.5, help='Seconds between API calls (default: 0.5)')
    parser.add_argument('--discord', action='store_true', help='Run as a Discord listener bot (download .json attachments and process automatically).')
    parser.add_argument(
        '--source-channel-id',
        type=int,
        default=int(os.environ.get('DISCORD_SOURCE_CHANNEL_ID', '1468351435451273320')),
        help='Discord channel ID to watch for .json uploads (default: env DISCORD_SOURCE_CHANNEL_ID or 1468351435451273320).',
    )
    parser.add_argument(
        '--dest-webhook-url',
        type=str,
        default=os.environ.get('DISCORD_DEST_WEBHOOK_URL', '').strip() or None,
        help='Discord incoming webhook URL to upload CSV results (default: env DISCORD_DEST_WEBHOOK_URL).',
    )
    parser.add_argument(
        '--downloads-dir',
        type=str,
        default=os.environ.get('DISCORD_DOWNLOADS_DIR', '').strip() or None,
        help='Folder to store downloaded JSON attachments (default: OUTPUT_DIR\\discord_downloads).',
    )
    args = parser.parse_args()

    # Prefer Discord mode when explicitly requested, or when no JSON file is given but a bot token exists.
    discord_bot_token = os.environ.get('DISCORD_BOT_TOKEN', '').strip()
    should_run_discord = bool(args.discord or ((args.json_file is None or not str(args.json_file).strip()) and discord_bot_token))

    json_file_path = args.json_file
    if should_run_discord:
        if not discord_bot_token:
            print('[ERROR] DISCORD_BOT_TOKEN is not set in environment.', file=sys.stderr)
            sys.exit(1)
        try:
            api_key = get_api_key()
        except ValueError as e:
            print(f'[ERROR] {e}', file=sys.stderr)
            sys.exit(1)

        downloads_dir = Path(args.downloads_dir) if args.downloads_dir else (OUTPUT_DIR / 'discord_downloads')
        try:
            asyncio.run(
                run_discord_listener(
                    discord_bot_token=discord_bot_token,
                    source_channel_id=int(args.source_channel_id),
                    output_dir=OUTPUT_DIR,
                    groq_api_key=api_key,
                    groq_model=args.model,
                    delay_between_calls=float(args.delay),
                    dest_webhook_url=args.dest_webhook_url,
                    downloads_dir=downloads_dir,
                )
            )
        except KeyboardInterrupt:
            print('\n[INFO] Stopped by user.')
        return

    if json_file_path is None or not str(json_file_path).strip():
        print('[ERROR] No JSON file path provided. Provide a JSON file argument, or run with --discord.', file=sys.stderr)
        sys.exit(1)

    json_path = Path(json_file_path)
    if not json_path.is_file():
        # If only a filename was given, search in INPUT_JSON_DIR first, then OUTPUT_DIR, script dir, cwd
        if not json_path.is_absolute() and len(json_path.parts) <= 1:
            script_dir = Path(__file__).resolve().parent
            for base in (INPUT_JSON_DIR, OUTPUT_DIR, script_dir, Path.cwd()):
                candidate = base / json_path
                if candidate.is_file():
                    json_path = candidate
                    break
        if not json_path.is_file():
            print(f'[ERROR] File not found: {json_file_path}', file=sys.stderr)
            print(f'  Searched in: {INPUT_JSON_DIR}, {OUTPUT_DIR}, script directory, and current directory.', file=sys.stderr)
            print('  Tip: Use the full path to the file if it is elsewhere.', file=sys.stderr)
            sys.exit(1)

    if args.output:
        output_csv_path = Path(args.output)
        if not output_csv_path.is_absolute():
            output_csv_path = OUTPUT_DIR / output_csv_path.name
    else:
        output_csv_path = OUTPUT_DIR / f'{json_path.stem}_cve_remediation.csv'

    try:
        api_key = get_api_key()
    except ValueError as e:
        print(f'[ERROR] {e}', file=sys.stderr)
        sys.exit(1)

    try:
        run_pipeline(json_path, api_key, args.model, output_csv_path, delay_between_calls=args.delay)
        print(f'\n[SUCCESS] Output CSV: {output_csv_path}')
    except Exception as e:
        print(f'[ERROR] {e}', file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
